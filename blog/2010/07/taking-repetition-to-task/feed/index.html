<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: Taking repetition to task</title>
	<atom:link href="http://antonymarcano.com/blog/2010/07/taking-repetition-to-task/feed/" rel="self" type="application/rss+xml" />
	<link>http://antonymarcano.com/blog/2010/07/taking-repetition-to-task/</link>
	<description>Thinking through writing... on innovation, business, technology and more</description>
	<lastBuildDate>Sun, 24 May 2015 01:54:00 +0000</lastBuildDate>
		<sy:updatePeriod>hourly</sy:updatePeriod>
		<sy:updateFrequency>1</sy:updateFrequency>
	<generator>http://wordpress.org/?v=4.0.7</generator>
	<item>
		<title>By: Antony Marcano</title>
		<link>http://antonymarcano.com/blog/2010/07/taking-repetition-to-task/comment-page-1/#comment-15</link>
		<dc:creator><![CDATA[Antony Marcano]]></dc:creator>
		<pubDate>Tue, 17 Aug 2010 12:23:23 +0000</pubDate>
		<guid isPermaLink="false">http://antonymarcano.com/blog/?p=21#comment-15</guid>
		<description><![CDATA[Thanks for that Michael.&lt;br&gt;&lt;br&gt;I agree that completed features is ultimately a far better measure of progress than just tests.&lt;br&gt;&lt;br&gt;Although my point was only that it makes *more* sense to track passing tests than expended effort on &#039;tasks&#039;.&lt;br&gt;&lt;br&gt;What I had in my mind was the typical measure of progress: tasks on a task-board and effort expended on a burn-up/down chart... I think a step in the right direction would be to place less emphasis on those things and measure something that gives us a better indication of progress. I think passing tests are a better indication of progress than tasks. Just like miles travelled/remaining on a journey is a better indication of progress than the amount or fuel-burned/remaining. Useful as that latter is, it isn&#039;t an indication of progress.&lt;br&gt;&lt;br&gt;I wasn&#039;t saying it was the best and only way of measuring progress :-)&lt;br&gt;&lt;br&gt;I&#039;ve not worked on any projects where automated checks are implemented entirely in advance of programming the feature... if people are doing that, they&#039;re not doing xDD (TDD/ATDD/BDD). These are iterative and incremental learning processes and, in my experience, do not work well if you try to specify all the checks in advance.&lt;br&gt;&lt;br&gt;Thanks again for the comment.]]></description>
		<content:encoded><![CDATA[<p>Thanks for that Michael.</p>
<p>I agree that completed features is ultimately a far better measure of progress than just tests.</p>
<p>Although my point was only that it makes *more* sense to track passing tests than expended effort on &#39;tasks&#39;.</p>
<p>What I had in my mind was the typical measure of progress: tasks on a task-board and effort expended on a burn-up/down chart&#8230; I think a step in the right direction would be to place less emphasis on those things and measure something that gives us a better indication of progress. I think passing tests are a better indication of progress than tasks. Just like miles travelled/remaining on a journey is a better indication of progress than the amount or fuel-burned/remaining. Useful as that latter is, it isn&#39;t an indication of progress.</p>
<p>I wasn&#39;t saying it was the best and only way of measuring progress <img src="http://antonymarcano.com/blog/wp-includes/images/smilies/icon_smile.gif" alt=":-)" class="wp-smiley" /></p>
<p>I&#39;ve not worked on any projects where automated checks are implemented entirely in advance of programming the feature&#8230; if people are doing that, they&#39;re not doing xDD (TDD/ATDD/BDD). These are iterative and incremental learning processes and, in my experience, do not work well if you try to specify all the checks in advance.</p>
<p>Thanks again for the comment.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Michael Bolton</title>
		<link>http://antonymarcano.com/blog/2010/07/taking-repetition-to-task/comment-page-1/#comment-10</link>
		<dc:creator><![CDATA[Michael Bolton]]></dc:creator>
		<pubDate>Thu, 15 Jul 2010 09:36:29 +0000</pubDate>
		<guid isPermaLink="false">http://antonymarcano.com/blog/?p=21#comment-10</guid>
		<description><![CDATA[&lt;i&gt;Surely it makes more sense to measure progress with passing tests (or “checks” – whichever you prefer).&lt;/i&gt;&lt;br&gt;&lt;br&gt;It doesn&#039;t make sense to measure progress with passing tests, whether you call them checks or not.  It might make sense to do that if the development of a software product were linear, or a set of pegs that you insert into a board.  But it isn&#039;t.&lt;br&gt;&lt;br&gt;A passing test (and in particular, a passing check) tells you that a product is capable of producing a correct result in a specific, highly controlled set of conditions.  It tells you that the product can do something.  It doesn&#039;t tell you, or even warn you, of terrible problems in the product of which the check is unaware.  For that, you need exploration.  As J.B. Rainsberger said once, a green bar doesn&#039;t tell you you&#039;re done; it tells you that you&#039;re ready for a real tester to kick the snot out of it.&lt;br&gt;&lt;br&gt;It does make sense, though, to measure progress in terms of completed features, which I think is what you&#039;re saying, where &quot;completed&quot; means that the feature has been developed and checked &lt;i&gt;and&lt;/i&gt; tested to the degree that a &lt;i&gt;person&lt;/i&gt; has deemed it acceptable.  I see great risk if the person has delegated that decision to checks alone, especially when the checks have been devised entirely in advance of the programming work on the feature.&lt;br&gt;&lt;br&gt;---Michael B.]]></description>
		<content:encoded><![CDATA[<p><i>Surely it makes more sense to measure progress with passing tests (or “checks” – whichever you prefer).</i></p>
<p>It doesn&#39;t make sense to measure progress with passing tests, whether you call them checks or not.  It might make sense to do that if the development of a software product were linear, or a set of pegs that you insert into a board.  But it isn&#39;t.</p>
<p>A passing test (and in particular, a passing check) tells you that a product is capable of producing a correct result in a specific, highly controlled set of conditions.  It tells you that the product can do something.  It doesn&#39;t tell you, or even warn you, of terrible problems in the product of which the check is unaware.  For that, you need exploration.  As J.B. Rainsberger said once, a green bar doesn&#39;t tell you you&#39;re done; it tells you that you&#39;re ready for a real tester to kick the snot out of it.</p>
<p>It does make sense, though, to measure progress in terms of completed features, which I think is what you&#39;re saying, where &#8220;completed&#8221; means that the feature has been developed and checked <i>and</i> tested to the degree that a <i>person</i> has deemed it acceptable.  I see great risk if the person has delegated that decision to checks alone, especially when the checks have been devised entirely in advance of the programming work on the feature.</p>
<p>&#8212;Michael B.</p>
]]></content:encoded>
	</item>
</channel>
</rss>
